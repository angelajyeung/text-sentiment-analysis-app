{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMwsyLx1m1a6ajP1cF3rsL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angelajyeung/text-sentiment-analysis-app/blob/main/model_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "test_data = pd.read_csv('test.csv')\n",
        "test_labels = pd.read_csv('test_labels.csv')\n",
        "test_data = pd.merge(test_data, test_labels, on='id')\n",
        "test_data = test_data[test_data['toxic'] != -1] # remove rows where the label is -1\n",
        "test_data = test_data.reset_index(drop=True)\n",
        "test_df = test_data.head(100)\n",
        "\n",
        "# Convert labels to multi-hot encoded format\n",
        "test_df['labels'] = test_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values.tolist()\n",
        "\n",
        "# Print the first 5 rows of the data frame\n",
        "print(test_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCTTpx3MNthu",
        "outputId": "00aacfa4-7bca-4879-f2bf-a71886e8be48"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 id                                       comment_text  toxic  \\\n",
            "0  0001ea8717f6de06  Thank you for understanding. I think very high...      0   \n",
            "1  000247e83dcc1211                   :Dear god this site is horrible.      0   \n",
            "2  0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...      0   \n",
            "3  0003e1cccfd5a40a  \" \\n\\n It says it right there that it IS a typ...      0   \n",
            "4  00059ace3e3e9a53  \" \\n\\n == Before adding a new product to the l...      0   \n",
            "\n",
            "   severe_toxic  obscene  threat  insult  identity_hate              labels  \n",
            "0             0        0       0       0              0  [0, 0, 0, 0, 0, 0]  \n",
            "1             0        0       0       0              0  [0, 0, 0, 0, 0, 0]  \n",
            "2             0        0       0       0              0  [0, 0, 0, 0, 0, 0]  \n",
            "3             0        0       0       0              0  [0, 0, 0, 0, 0, 0]  \n",
            "4             0        0       0       0              0  [0, 0, 0, 0, 0, 0]  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-9c20ef88dfac>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df['labels'] = test_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values.tolist()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "\n",
        "# Load and preprocess the training data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "train_data = train_data.head(100)\n",
        "train_data = train_data.fillna('') # replace missing values with empty strings\n",
        "train_data['text'] = train_data['comment_text'].str.lower().str.strip() # lowercase and remove leading/trailing spaces from text\n",
        "train_data['labels'] = train_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values.tolist() # create a new column for multi-hot encoded labels\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Define the datasets and data collator\n",
        "class ToxicCommentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        comment_text = str(self.data.iloc[index]['comment_text'])\n",
        "        labels = self.data.iloc[index]['labels']\n",
        "        id = self.data.iloc[index]['id']\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            comment_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            return_token_type_ids=True,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'id': id,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
        "            'labels': torch.tensor(labels, dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "train_dataset = ToxicCommentDataset(train_data, tokenizer)\n",
        "val_dataset = ToxicCommentDataset(val_data, tokenizer)\n",
        "\n",
        "def data_collator(data):\n",
        "    batch = {}\n",
        "    batch['input_ids'] = torch.stack([item['input_ids'] for item in data])\n",
        "    batch['attention_mask'] = torch.stack([item['attention_mask'] for item in data])\n",
        "    batch['token_type_ids'] = torch.stack([item['token_type_ids'] for item in data])\n",
        "    batch['labels'] = torch.stack([item['labels'] for item in data])\n",
        "    return batch\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)\n"
      ],
      "metadata": {
        "id": "9nBggUyzOpGH"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Load the pre-trained BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)\n",
        "\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=50,\n",
        "    learning_rate=2e-5,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_roc_auc_score'\n",
        ")\n",
        "\n",
        "# Define the optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=training_args.learning_rate, eps=1e-8)\n",
        "total_steps = len(train_dataloader) * training_args.num_train_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=training_args.warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "# Define the evaluation metrics\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    roc_auc = roc_auc_score(labels, pred.predictions, average='weighted', multi_class='ovr')\n",
        "    return {'f1_score': f1, 'accuracy': accuracy, 'roc_auc_score': roc_auc}\n",
        "\n",
        "# Define the Trainer object and start the training\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    optimizers=(optimizer, scheduler)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obgf7yfPO89f",
        "outputId": "142fc92f-0031-4d72-a405-fd1d363d1d55"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "t6RbawdrUroB",
        "outputId": "1eca0699-23cd-45c7-d31d-c212a9a70548"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 05:02, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=15, training_loss=0.7705146948496501, metrics={'train_runtime': 336.0987, 'train_samples_per_second': 0.714, 'train_steps_per_second': 0.045, 'total_flos': 15787230289920.0, 'train_loss': 0.7705146948496501, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data and preprocess it\n",
        "test_data = pd.read_csv('test.csv')\n",
        "test_data = test_data.fillna('')\n",
        "test_data['text'] = test_data['comment_text'].str.lower().str.strip()\n",
        "\n",
        "# Define the test dataset and data collator\n",
        "class ToxicCommentTestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.labels = torch.zeros((len(self.data), 6), dtype=torch.float32)  # create a dummy labels column with all zeros\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        comment_text = str(self.data.iloc[index]['comment_text'])\n",
        "        id = self.data.iloc[index]['id']\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            comment_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            return_token_type_ids=True,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'id': id,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
        "            'labels': self.labels[index]\n",
        "        }\n",
        "\n",
        "test_dataset = ToxicCommentTestDataset(test_df, tokenizer)\n",
        "# Evaluate the model on the test dataset\n",
        "# test_predictions = trainer.predict(test_dataset)"
      ],
      "metadata": {
        "id": "bo0LsUwJf_RE"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}